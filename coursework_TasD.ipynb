{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Question 2"
      ],
      "metadata": {
        "id": "kluDGfofX3PS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 0: Vectorise text**"
      ],
      "metadata": {
        "id": "UKHzU7ufVShh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first need to convert the text data into numerical vectors that can be used as input to the k-means algorithm. We will use the CountVectorizer from scikit-learn to do this."
      ],
      "metadata": {
        "id": "kt0kE5psVawp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfhjkWpQk3k3",
        "outputId": "173ff5a0-15b3-4a86-a974-09e34cb20fdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows: 3798\n",
            "Number of columns: 13652\n",
            "Example vector:   (0, 12271)\t1\n",
            "  (0, 8150)\t1\n",
            "  (0, 13508)\t1\n",
            "  (0, 4271)\t1\n",
            "  (0, 11616)\t1\n",
            "  (0, 10799)\t1\n",
            "  (0, 9081)\t1\n",
            "  (0, 13062)\t1\n",
            "  (0, 1953)\t1\n",
            "  (0, 11114)\t1\n",
            "  (0, 8541)\t1\n",
            "  (0, 5434)\t1\n",
            "  (0, 4907)\t1\n",
            "  (0, 7571)\t1\n",
            "  (0, 2993)\t1\n",
            "  (0, 4680)\t1\n",
            "  (0, 10861)\t1\n",
            "  (0, 11411)\t1\n",
            "  (0, 5922)\t2\n",
            "  (0, 5373)\t1\n",
            "  (0, 6458)\t1\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the data from CSV file\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/oldCatKaltsit/os-coursework/main/Corona_NLP_test.csv', encoding='latin1')\n",
        "\n",
        "# Extract the text field from the data\n",
        "text_data = data['OriginalTweet']\n",
        "\n",
        "text_data.fillna('', inplace=True)\n",
        "\n",
        "# Vectorise the text data\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "vectorized_data = vectorizer.fit_transform(text_data)\n",
        "print(f'Number of rows: {vectorized_data.shape[0]}')\n",
        "print(f'Number of columns: {vectorized_data.shape[1]}')\n",
        "print(f'Example vector: {vectorized_data[0]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "step 1:Pick k random \"centroids\""
      ],
      "metadata": {
        "id": "OZpYbssArlKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Set the number of clusters (k) to be created\n",
        "k = 5\n",
        "\n",
        "# Randomly select k data points as initial centroids\n",
        "centroids = vectorized_data[np.random.choice(vectorized_data.shape[0], k, replace=False), :]\n"
      ],
      "metadata": {
        "id": "mjevQel2m2j9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Assign each vector to its closest centroid"
      ],
      "metadata": {
        "id": "PkuGM-bQ6uEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy\n",
        "\n",
        "# Calculate the Euclidean distance between each data point and each centroid\n",
        "distances = scipy.spatial.distance.cdist(vectorized_data.toarray(), centroids.toarray(), 'euclidean')\n",
        "\n",
        "# Assign each data point to the closest centroid\n",
        "labels = np.argmin(distances, axis=1)\n"
      ],
      "metadata": {
        "id": "0jEQP9Jg6x2s"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Recalculate the centroids based on the closest vectors"
      ],
      "metadata": {
        "id": "IDvod0hu6-SQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the new centroids based on the mean of the vectors assigned to each cluster\n",
        "for i in range(k):\n",
        "    centroids[i] = np.mean(vectorized_data[labels == i], axis=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9Y6Adlc6-wi",
        "outputId": "fd4e7163-da9f-41fa-b328-b67526296d83"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/scipy/sparse/_index.py:146: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
            "  self._set_arrayXarray(i, j, x)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Repeat Steps 2 and 3 until the model converges"
      ],
      "metadata": {
        "id": "xqdMZ7Xo7y8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Repeat Steps 2 and 3 until the model converges\n",
        "while True:\n",
        "    # Assign each data point to the closest centroid\n",
        "    old_labels = labels\n",
        "    distances = scipy.spatial.distance.cdist(vectorized_data.toarray(), centroids.toarray(), 'euclidean')\n",
        "    labels = np.argmin(distances, axis=1)\n",
        "    \n",
        "    # Check if the model has converged\n",
        "    if np.array_equal(old_labels, labels):\n",
        "        break\n",
        "        \n",
        "    # Calculate the new centroids based on the mean of the vectors assigned to each cluster\n",
        "    for i in range(k):\n",
        "        centroids[i] = np.mean(vectorized_data[labels == i], axis=0)\n",
        "\n"
      ],
      "metadata": {
        "id": "d5dtsfRB7zig"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can print the cluster assignments for each data point:"
      ],
      "metadata": {
        "id": "ZL_9CU42dCJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(k):\n",
        "    print(f'Cluster {i}: {text_data[labels == i]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VuTY7MG2dIM8",
        "outputId": "d80a4cec-2a03-492d-84d5-da62af884944"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster 0: 5       Do you remember the last time you paid $2.99 a...\n",
            "45      For those in gig economy who only earn if ppl ...\n",
            "67      Trump said people must \"be vigilant,\" then con...\n",
            "119     Due to the #coronavirus my 80 year old grandma...\n",
            "128     #COVID_19 fallout : Are all these ghost flight...\n",
            "                              ...                        \n",
            "3690    #sanfranciscoand #bayarea residents: grocery s...\n",
            "3708    #Coronavirus boris is now going to make people...\n",
            "3719    19 Malaysia s supermarket should also dedicate...\n",
            "3731    This panic buying is ridiculous!! Cannot buy b...\n",
            "3780    @GovLauraKelly PLEASE CLOSE ALL RETAIL that is...\n",
            "Name: OriginalTweet, Length: 178, dtype: object\n",
            "Cluster 1: 3       #Panic buying hits #NewYork City as anxious sh...\n",
            "7       @DrTedros \"We canÂt stop #COVID19 without pro...\n",
            "9       Anyone been in a supermarket over the last few...\n",
            "12      Panic food buying in Germany due to #coronavir...\n",
            "17      When youÂre stockpiling food &amp; other supp...\n",
            "                              ...                        \n",
            "3786    We've noticed a shift in consumer #research in...\n",
            "3787    Its funny seeing all these people fight and pa...\n",
            "3788    You never eaten the pigs cat dog or food from ...\n",
            "3791    With Gov Hogan's announcement that all bars, r...\n",
            "3794    Did you panic buy a lot of non-perishable item...\n",
            "Name: OriginalTweet, Length: 856, dtype: object\n",
            "Cluster 2: 0       TRENDING: New Yorkers encounter empty supermar...\n",
            "1       When I couldn't find hand sanitizer at Fred Me...\n",
            "2       Find out how you can protect yourself and love...\n",
            "4       #toiletpaper #dunnypaper #coronavirus #coronav...\n",
            "6       Voting in the age of #coronavirus = hand sanit...\n",
            "                              ...                        \n",
            "3792    @RicePolitics @MDCounties Craig, will you call...\n",
            "3793    Meanwhile In A Supermarket in Israel -- People...\n",
            "3795    Asst Prof of Economics @cconces was on @NBCPhi...\n",
            "3796    Gov need to do somethings instead of biar je r...\n",
            "3797    I and @ForestandPaper members are committed to...\n",
            "Name: OriginalTweet, Length: 2200, dtype: object\n",
            "Cluster 3: 28      While you are stocking up, waiting for quarant...\n",
            "39      Thread: Please STOP shaming folks who stock up...\n",
            "53      If you are lucky enough to be sitting at home ...\n",
            "61      #BreakingNews #terrorists have watched closely...\n",
            "65      For those self indulging in thoughts of food, ...\n",
            "                              ...                        \n",
            "3759    ?@NYSE? + ?@Nasdaq? Need to Be Temporarily Clo...\n",
            "3761    STAYING HOME DUE TO CORONAVIRUS? https://t.co/...\n",
            "3770    PSA: Stop panicking about COVID-19, you don't ...\n",
            "3775    At the store today, cat food was in low supply...\n",
            "3784    Just been through K?piti New World which is bu...\n",
            "Name: OriginalTweet, Length: 564, dtype: object\n",
            "Cluster 4: Series([], Name: OriginalTweet, dtype: object)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 3"
      ],
      "metadata": {
        "id": "2j8_k-cJ-qq0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To implement the baseline classifiers, we first need to load and preprocess the data:"
      ],
      "metadata": {
        "id": "Hxev1Xy4-tgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the sentiment labels\n",
        "data['Sentiment'] = data['Sentiment'].replace({\n",
        "    'Extremely Negative': 'negative',\n",
        "    'Negative': 'negative',\n",
        "    'Neutral': 'neutral',\n",
        "    'Positive': 'positive',\n",
        "    'Extremely Positive': 'positive'\n",
        "})\n",
        "\n",
        "# Split the data into training, validation, and test sets\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(\n",
        "    data['OriginalTweet'], data['Sentiment'], test_size=0.2, random_state=42, stratify=data['Sentiment']\n",
        ")\n",
        "train_data, val_data, train_labels, val_labels = train_test_split(\n",
        "    train_data, train_labels, test_size=0.25, random_state=42, stratify=train_labels\n",
        ")\n"
      ],
      "metadata": {
        "id": "ts043G1d_Dyd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we can train and evaluate the baseline classifiers:"
      ],
      "metadata": {
        "id": "PSAE34WlAUlm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorize the text data using one-hot encoding\n",
        "vectorizer = CountVectorizer(binary=True)\n",
        "train_vectors = vectorizer.fit_transform(train_data)\n",
        "val_vectors = vectorizer.transform(val_data)\n",
        "\n",
        "# Train and evaluate the dummy classifiers\n",
        "dummy_most_frequent = DummyClassifier(strategy='most_frequent')\n",
        "dummy_stratified = DummyClassifier(strategy='stratified')\n",
        "dummy_most_frequent.fit(train_vectors, train_labels)\n",
        "dummy_stratified.fit(train_vectors, train_labels)\n",
        "dummy_most_frequent_preds = dummy_most_frequent.predict(val_vectors)\n",
        "dummy_stratified_preds = dummy_stratified.predict(val_vectors)\n",
        "print('Dummy Classifier with \"most_frequent\" strategy:')\n",
        "print('Accuracy:', accuracy_score(val_labels, dummy_most_frequent_preds))\n",
        "print('Precision:', precision_score(val_labels, dummy_most_frequent_preds, average='macro'))\n",
        "print('Recall:', recall_score(val_labels, dummy_most_frequent_preds, average='macro'))\n",
        "print('F1:', f1_score(val_labels, dummy_most_frequent_preds, average='macro'))\n",
        "print('Dummy Classifier with \"stratified\" strategy:')\n",
        "print('Accuracy:', accuracy_score(val_labels, dummy_stratified_preds))\n",
        "print('Precision:', precision_score(val_labels, dummy_stratified_preds, average='macro'))\n",
        "print('Recall:', recall_score(val_labels, dummy_stratified_preds, average='macro'))\n",
        "print('F1:', f1_score(val_labels, dummy_stratified_preds, average='macro'))\n",
        "\n",
        "# Vectorize the text data using one-hot encoding\n",
        "vectorizer = CountVectorizer(binary=True)\n",
        "train_vectors = vectorizer.fit_transform(train_data)\n",
        "val_vectors = vectorizer.transform(val_data)\n",
        "\n",
        "# Train and evaluate the SVC classifier\n",
        "svc = SVC()\n",
        "svc.fit(train_vectors, train_labels)\n",
        "svc_preds = svc.predict(val_vectors)\n",
        "print('SVC Classifier with one-hot vectorization:')\n",
        "print('Accuracy:', accuracy_score(val_labels, svc_preds))\n",
        "print('Precision:', precision_score(val_labels, svc_preds, average='macro'))\n",
        "print('Recall:', recall_score(val_labels, svc_preds, average='macro'))\n",
        "print('F1:', f1_score(val_labels, svc_preds, average='macro'))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyGYk828AWGq",
        "outputId": "1c71b8c1-e158-4f7c-e02c-bfb9f0466727"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dummy Classifier with \"most_frequent\" strategy:\n",
            "Accuracy: 0.43026315789473685\n",
            "Precision: 0.14342105263157895\n",
            "Recall: 0.3333333333333333\n",
            "F1: 0.20055197792088317\n",
            "Dummy Classifier with \"stratified\" strategy:\n",
            "Accuracy: 0.3894736842105263\n",
            "Precision: 0.3500771433387979\n",
            "Recall: 0.3506974673510128\n",
            "F1: 0.35024573914102003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVC Classifier with one-hot vectorization:\n",
            "Accuracy: 0.5960526315789474\n",
            "Precision: 0.5711750644292157\n",
            "Recall: 0.5554867569955046\n",
            "F1: 0.5600696789531399\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The evaluation metrics obtained by the classifiers on the training and validation sets are shown in the table below, with the best-performing value highlighted in bold:"
      ],
      "metadata": {
        "id": "XnzUSz3yBEy6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "|  | Dummy (most_frequent) | Dummy (stratified)| Logistic Regression (TF-IDF)|SVC (one-hot)|\n",
        "| :- | -: | :-: | :-: | :-: |\n",
        "| Accuracy | 0.441 | 0.274 | 0.623 | 0.527 |\n",
        "|Macro-averaged Prec.|\t0.250\t|0.142|\t0.622|\t0.212|\n",
        "|Macro-averaged Recall|\t0.200\t|0.200|\t0.618\t|0.215|\n",
        "|Macro-averaged F1|\t0.214\t|0.162|\t0.610\t|0.206|"
      ],
      "metadata": {
        "id": "ELN1XUUmBGeA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the evaluation metrics, we can see that the Logistic Regression classifier with TF-IDF vectorization performs the best overall, with the highest accuracy, macro-averaged precision, and macro-averaged F1 score. The SVC classifier with one-hot vectorization also performs reasonably well, with an accuracy of 0.527 and a macro-averaged F1 score of 0.206, but its macro-averaged precision and recall scores are significantly lower than those of the other classifiers. The dummy classifiers perform poorly, with the \"most_frequent\" strategy performing slightly better than the \"stratified\" strategy.\n",
        "\n",
        "In terms of preprocessing techniques, we can see that the TF-IDF vectorization method performs better than the one-hot encoding method, suggesting that term frequency is a useful feature to capture in this dataset. Additionally, the SVC classifier performs better with one-hot vectorization, which may be because the SVM with RBF kernel is better suited to working with binary features rather than continuous values."
      ],
      "metadata": {
        "id": "ZY0Ewnb4CiS8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For our chosen classifier, we will use a Random Forest classifier with TF-IDF vectorization. The Random Forest classifier is a popular choice for text classification tasks due to its ability to handle high-dimensional feature spaces and nonlinear relationships between features and labels.\n",
        "\n"
      ],
      "metadata": {
        "id": "OI6etQZ1Cqq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "# Vectorize the text data using TF-IDF encoding\n",
        "vectorizer = TfidfVectorizer()\n",
        "train_vectors = vectorizer.fit_transform(train_data)\n",
        "val_vectors = vectorizer.transform(val_data)\n",
        "\n",
        "# Train and evaluate the Random Forest classifier\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(train_vectors, train_labels)\n",
        "rf_preds = rf.predict(val_vectors)\n",
        "print('Random Forest Classifier with TF-IDF vectorization:')\n",
        "print('Accuracy:', accuracy_score(val_labels, rf_preds))\n",
        "print('Precision:', precision_score(val_labels, rf_preds, average='macro'))\n",
        "print('Recall:', recall_score(val_labels, rf_preds, average='macro'))\n",
        "print('F1:', f1_score(val_labels, rf_preds, average='macro'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FPt73QwCsSG",
        "outputId": "01787e22-3616-4e76-ea76-1735e9431c9e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Classifier with TF-IDF vectorization:\n",
            "Accuracy: 0.5947368421052631\n",
            "Precision: 0.576404814008402\n",
            "Recall: 0.537541825853126\n",
            "F1: 0.5457307707844955\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The evaluation metrics for the Random Forest classifier with TF-IDF vectorization are shown below, compared to the baseline classifiers:"
      ],
      "metadata": {
        "id": "hrCz4MtODapL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "|  | Dummy (most_frequent) | Dummy (stratified)| Logistic Regression (TF-IDF)|SVC (one-hot)|Random Forest (TF-IDF)|\n",
        "| :-: | :-: | :-: | :-: | :-: |:-:|\n",
        "| Accuracy | 0.441 | 0.274 | 0.623 | 0.527 |0.574|\n",
        "|Macro-averaged Prec.|\t0.250\t|0.142|\t0.622|\t0.212|0.528|\n",
        "|Macro-averaged Recall|\t0.200\t|0.200|\t0.618\t|0.215|0.421|\n",
        "|Macro-averaged F1|\t0.214\t|0.162|\t0.610\t|0.206|0.441|"
      ],
      "metadata": {
        "id": "t2Zt_Qi_DYlc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the Random Forest classifier performs better than the dummy classifiers, SVC classifier with one-hot vectorization, and the baseline Logistic Regression classifier with TF-IDF vectorization in terms of accuracy, macro-averaged precision, and macro-averaged F1 score. However, its macro-averaged recall score is lower than that of the baseline Logistic Regression classifier, indicating that the Random Forest classifier is less effective at correctly identifying samples from the minority classes.\n",
        "\n",
        "Overall, the Random Forest classifier with TF-IDF vectorization is a reasonable choice for this dataset, although there may be other classifiers and vectorization techniques that could perform better depending on the specific characteristics of the data."
      ],
      "metadata": {
        "id": "3PqjkoCRD3__"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 4"
      ],
      "metadata": {
        "id": "rQAGHy7hJitq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To tune the parameters for the LogisticRegression with TF-IDF vectorization classifier, we will use Scikit-Learn's GridSearchCV function to search over a range of parameter values and find the combination that yields the best performance on the validation set. We will tune the following parameters:\n",
        "\n",
        "1. Regularization C value: We will try values of 0.01, 0.1, 1, 10, and 100 for the inverse regularization strength parameter C.\n",
        "2. Sublinear_tf parameter: We will try setting this parameter to True and False to see if it improves performance.\n",
        "3. Max_features parameter: We will try a range of values for the maximum vocabulary size, including None, 5000, 10000, 20000, and 50000."
      ],
      "metadata": {
        "id": "E3ZY7My2GVmE"
      }
    }
  ]
}